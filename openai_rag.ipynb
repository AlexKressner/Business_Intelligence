{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKressner/Business_Intelligence/blob/main/openai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q cohere tiktoken openai langchain langchain-community langchain-openai pypdf faiss-cpu"
      ],
      "metadata": {
        "id": "qkVcM1Z_UdVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk71q6XNUbuD"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from IPython.display import display, HTML\n",
        "import pypdf\n",
        "import re\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/AlexKressner/Business_Intelligence"
      ],
      "metadata": {
        "id": "366CDfr1WcdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secret_key = \"\""
      ],
      "metadata": {
        "id": "Fz7EjjfXXt-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlo9aj0DUbuE"
      },
      "source": [
        "# LangChain\n",
        "LangChain ist ein Entwicklungsframework für Anwendungen, die auf Sprachmodellen basieren. Es ermöglicht die Erstellung von Anwendungen, die kontextbewusst sind, indem es ein Sprachmodell mit verschiedenen Kontextquellen verbindet, wie Anweisungen, Beispielen und Inhalten, die als Grundlage für Antworten dienen. Zusätzlich unterstützt LangChain die Fähigkeit des Sprachmodells, logisch zu überlegen, um auf Basis des gegebenen Kontexts Entscheidungen zu treffen und entsprechende Aktionen zu initiieren. Weitere Informationen finden Sie unter https://python.langchain.com/docs/get_started/introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPL02bBNUbuF"
      },
      "source": [
        "# Retrival Augmented Generation\n",
        "Viele Anwendungen von großen Sprachmodellen (LLMs) erfordern nutzerspezifische Daten, die nicht Teil des Trainingsdatensatzes des Modells sind. Eine aktuell sehr beachtete Methode, um dies zu erreichen, ist durch Retrieval Augmented Generation (RAG). In diesem Prozess werden für den Anwendungskontext relevante Daten abgerufen und dem LLM zur Textgeneration übergeben. Wir werden diese Methode in einem einfachen Beispiel gemeinsam anwenden. Dazu wollen wir eine Anwendung entwickeln, die es dem Nutzer ermöglicht sich über das Europawahlprogramm einer deutschen Partei zu informieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tU-OrzUbuF"
      },
      "source": [
        "<img src=\"https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg\" width=1200 height=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpiHNdLSUbuG"
      },
      "source": [
        "## 1 Document Loader\n",
        "Mithilfe von sogenannten [Document Loadern](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.document_loaders) können Sie Dokumente aus vielen verschiedenen Quellen laden. LangChain bietet über 100 verschiedene Dokumentenlader, mit denen Sie alle Arten von Dokumenten (HTML, PDF, Code) aus allen Arten von Standorten (private S3-Buckets, öffentliche Websites) laden können."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls Business_Intelligence"
      ],
      "metadata": {
        "id": "FxPZAQJQW7iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7N2BvTNUbuG"
      },
      "outputs": [],
      "source": [
        "DOCUMENT = \"Business_Intelligence/Daten/RAG/FDP.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FghN8PvVUbuG"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(DOCUMENT)\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9hAx7UdUbuG"
      },
      "outputs": [],
      "source": [
        "print(doc[5].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZu4EMBUbuH"
      },
      "source": [
        "## 2 Text Splitting\n",
        "Ein wesentlicher Teil von RAG besteht darin, nur die relevanten Teile von Dokumenten abzurufen. Dies umfasst mehrere Transformationsschritte, um die Dokumente für die Wiedergewinnung vorzubereiten. Einer der wichtigsten Schritte hierbei ist das Aufteilen (oder Zerlegen) eines großen Dokuments in kleinere Abschnitte. LangChain bietet mehrere Transformationsalgorithmen dafür, sowie für spezifische Dokumententypen (Code, Markdown usw.) optimierte Logik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgzqslOTUbuH"
      },
      "outputs": [],
      "source": [
        "# helper function\n",
        "def display_colored_chunks(docs:[str]):\n",
        "    concatenated_string = \"\"\n",
        "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\"]  # Specify the font colors\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        chunk = f'<span style=\"color:{colors[i % len(colors)]}\">{doc}</span>'\n",
        "        concatenated_string += chunk\n",
        "\n",
        "    # Display the concatenated string\n",
        "    display(HTML(concatenated_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5te3rEb2UbuH"
      },
      "source": [
        "### 2.1 Langchain Text Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGpxuL4XUbuI"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "docs = text_splitter.split_documents(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs2jlJ8PUbuI"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdQ87e1UUbuI"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks([doc.page_content for doc in docs[:5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es besteht die Möglichkeit diverse Optionen für einen Text Splitter zu setzen. Zum Beispiel können Sie die Größe der aus einem Split resultierenden Textbausteine (Chunks) definieren und den Umfang der Überlappung zwischen zwei Textbausteinen (Overlap)."
      ],
      "metadata": {
        "id": "yGUk1U8AZhMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*PoFhwSJf5_pa7ZjXG25gtg.png\" width=600 height=200 />"
      ],
      "metadata": {
        "id": "ieKIg-JqaIYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m651C93CUbuJ"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(doc)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XGnOTjNUbuJ"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks([doc.page_content for doc in docs[:5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyyX0g31UbuJ"
      },
      "source": [
        "### 2.2 Custom TextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad-2cOnaUbuJ"
      },
      "outputs": [],
      "source": [
        "# Open the PDF file\n",
        "with open(DOCUMENT, 'rb') as file:\n",
        "    # Create a PDF reader object\n",
        "    pdf_reader = pypdf.PdfReader(file)\n",
        "\n",
        "    # Initialize an empty string to store the pages\n",
        "    all_pages = \"\"\n",
        "\n",
        "    # Iterate over each page in the PDF\n",
        "    for page in pdf_reader.pages:\n",
        "        # Extract the text from the page and append it to the all_pages string\n",
        "        all_pages += page.extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(all_pages[:5000])"
      ],
      "metadata": {
        "id": "2bQ247ARbB_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaEUhaUZUbuK"
      },
      "outputs": [],
      "source": [
        "# Aufteilen in Chunks nach einem definierten Muster (\"Leerzeichen-Leerzeichen-Neue_Zeile\")\n",
        "chunks = re.split(r\"\\s\\s\\n\", all_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QTb2vYnUbuK"
      },
      "outputs": [],
      "source": [
        "docs = [Document(page_content=chunk) for chunk in chunks if len(chunk) > 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ntOiofUbuK"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzTm2zVaUbuL"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks([doc.page_content for doc in docs[:10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwGvYdDUbuL"
      },
      "source": [
        "## 3 Text Embedding & Vector Store\n",
        "Ein weiterer wichtiger Teil des Retrievals ist das Erstellen von Embeddings für Dokumente. Embeddings erfassen die semantische Bedeutung des Textes, was es Ihnen ermöglicht, schnell und effizient andere Textteile zu finden, die ähnlich sind. Nachfolgend finden Sie ein einfaches Embedding von Wörtern als Vektoren. Beispielsweise wird dem Wort `man` der Vektor `[1,7]` und dem Wort `woman` der Vektor `[9,7]` zugeordnet.\n",
        "\n",
        "<img src=\"https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/figures/fig5.png\" width=650 height=500 />\n",
        "\n",
        "Mit dem Aufkommen von Embeddings entstand der Bedarf nach Datenbanken, die eine effiziente Speicherung und Suche dieser besonderen Datenstruktur unterstützt. Regelmäßig werden dafür sogenannte Vektordatenbanken verwendet. Wichtige Elemente von Vektordatenbanken sind die **Indexinerung** und das **Abfragen** von in der Datenbank gespeicherten Vektoren.\n",
        "\n",
        "1. **Indexierung**: Die abzuspeichernden Vektoren sind typischer Weise sehr groß. Aus diesem Grund versucht man, die ursprünglichen Vektoren in einer komprimierten Repräsentation abzubilden. Diese wird dann Vektorindex genannt und erlaubt das schnellere Abfragen von Vektoren.\n",
        "\n",
        "2. **Abfrage**: Zentrales Element bei der Abfrage von Vektoren einer Vektordatenbank sind sogenannte Ähnlichkeitsmaße. Diese Maße sind die Grundlage dafür, wie eine Vektordatenbank vergleicht und die relevantesten Ergebnisse für eine gegebene Anfrage identifiziert. Ähnlichkeitsmaße sind mathematische Methoden, um zu bestimmen, wie ähnlich zwei Vektoren in einem Vektorraum sind. Sie werden verwendet, um die in der Datenbank gespeicherten Vektoren zu vergleichen und die zu finden, die einem gegebenen Anfragevektor am ähnlichsten sind. Es gibt mehrere Ähnlichkeitsmaße z.B. Kosinusähnlichkeit, euklidische Distanz und Skalarprodukt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpX8HI7jUbuL"
      },
      "source": [
        "### 3.1 Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Qv1OclUbuL"
      },
      "outputs": [],
      "source": [
        "words = [\"my favorite animal is a dog\", \"i like cats the most\", \"the soccer game yesterday was great\"]\n",
        "embeddings = OpenAIEmbeddings(api_key=secret_key).embed_documents(words)\n",
        "len(embeddings), len(embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nmBGKzGUbuM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate cosine distances between elements of the embedding\n",
        "distances = cosine_similarity(embeddings)\n",
        "\n",
        "pd.DataFrame(distances, index=words, columns=words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrmTmh4SUbuM"
      },
      "source": [
        "### 3.2 Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxi6b8FRUbuM"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(docs, OpenAIEmbeddings(api_key=secret_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNn4VvH4UbuM"
      },
      "outputs": [],
      "source": [
        "prompt = \"Wie ist die Position der Partei zur europäischen Außenpolitik?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdElyXiIUbuM"
      },
      "outputs": [],
      "source": [
        "chunks = db.similarity_search(prompt, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii8FxuB-UbuM"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks([chunk.page_content for chunk in chunks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rv2vRIUUbuM"
      },
      "source": [
        "## 4 Retrieval\n",
        "Ein Retriever ist eine Schnittstelle, die Dokumente anhand einer unstrukturierten Anfrage zurückgibt. Es gibt hierbei verschiedene Algorithmen, um geeignete Dokumente zu laden. Wir werden zwei davon betrachten: **Maximum Marginal Relevance** und **Contextual Compression**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WudpVRAXUbuN"
      },
      "source": [
        "### 4.1 Maximum Marginal Relevance (MMR)\n",
        "Beim MMR Algorithmus werden zunächst basierend auf semantischer Ähnlichkeit die `fetch_k` ähnlichsten Vektoren gesucht und anschließend die `k` diversesten zurückgebenen.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*AGmzHH_imqwxgMqnss5PLQ.png\" width=700 height=300 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyM3XzilUbuN"
      },
      "outputs": [],
      "source": [
        "chunks = db.similarity_search(prompt, k=2)\n",
        "display_colored_chunks([chunk.page_content for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aVR2P4IUbuN"
      },
      "outputs": [],
      "source": [
        "chunks = db.max_marginal_relevance_search(prompt, k=2, fetch_k=5)\n",
        "display_colored_chunks([chunk.page_content for chunk in chunks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704YBwsNUbuN"
      },
      "source": [
        "### 4.2 Contextual Compression\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*HsP17K2tcnVm3H2ififsag.png\" width=750 height=480 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NulBAiH1UbuN"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=secret_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhxjgIT_UbuN"
      },
      "outputs": [],
      "source": [
        "def compress_retrieved_text(context_for_compression:str, text_for_compression:str):\n",
        "    compression_prompt = f\"\"\"\n",
        "    Du erhälst nachfolgend in einfachen Anführungszeichen einen Text. Bitte nehme aus diesem Text nur die Informationen heraus, die für die folgende Frage relevant sind: {context_for_compression}.\n",
        "    Bitte füge keine Informationen hinzu, die nicht im Text enthalten sind. '{text_for_compression}'\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": compression_prompt}],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMViXqcBUbuN"
      },
      "outputs": [],
      "source": [
        "chunks = db.similarity_search(prompt, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxvrLKAAUbuO"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks([chunk.page_content for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0tjyLKvUbuO"
      },
      "outputs": [],
      "source": [
        "compressed_chunks = []\n",
        "for chunk in chunks:\n",
        "    compressed_chunks.append(compress_retrieved_text(prompt, chunk.page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvVBLj6dUbuO"
      },
      "outputs": [],
      "source": [
        "display_colored_chunks(compressed_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSxKT8cUbuO"
      },
      "source": [
        "## 5 Beantwortung der Frage mit Kontext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfBXTV3oUbuO"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "docs = text_splitter.split_documents(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA4b_nhIUbuO"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMP8dSYrUbuP"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(docs, OpenAIEmbeddings(api_key=secret_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8bpL2yaUbuP"
      },
      "outputs": [],
      "source": [
        "prompt = \"Wie ist die Position der Partei zur Energiewende?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEArdY1UbuP"
      },
      "outputs": [],
      "source": [
        "def define_system_message(prompt:str, k:int):\n",
        "    chunks = db.similarity_search(prompt, k=k)\n",
        "    context = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "    system_message = f\"\"\"\n",
        "    Du bist ein freundlicher und hilfsbereiter Assistent. Du erhälst in einfachen Anführungszeichen einen Auszug aus dem Wahlprogramm einer\n",
        "    politischen Partei für die Europawahl. Du erhälst von einem Nutzer Fragen zu diesem Programmauszug. Deine Aufgabe ist es,\n",
        "    auf die Frage des Nutzers zu antworten und bei deiner Antwort ausschließlich den Text des bereitsgestellten Wahlprogramms zu verwenden.\n",
        "    Du darfst keine eigenen Meinungen oder Informationen hinzufügen. Du darfst auch keine Informationen aus anderen Quellen verwenden.\n",
        "    Bitte antworte sachlich und neutral. Der Textauszug lautet wie folgt: '{\" \".join(context)}'\n",
        "    \"\"\"\n",
        "\n",
        "    return system_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VMoNAiIUbuP"
      },
      "outputs": [],
      "source": [
        "# Hilfsfunktion zur Interaktion mit der Chat-API\n",
        "def get_completion(prompt:str, k:int=3, model:str=\"gpt-3.5-turbo\"):\n",
        "    system_message = [{\"role\": \"system\", \"content\": define_system_message(prompt, k)}] # Wie soll sich das System grundlegend verhalten\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}] # Prompt des Nutzers\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=system_message + messages,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4mJRW5DUbuP"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp3j5KntUbuP"
      },
      "outputs": [],
      "source": [
        "pprint(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaaIkv4wUbuP"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9w5xJvYUbuP"
      },
      "outputs": [],
      "source": [
        "pprint(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aufgabe\n",
        "1. Welche praktischen Anwendungen sehen Sie in Ihrem Unternehmen für Retrival Augmented Generation (RAG), d.h. welche Geschäftsprozesse lassen sich durch ein Large Language Model in Verbindung mit unternehmensspezifischen Daten verbessern? Bitte nehmen Sie sich etwas Zeit und disktuieren Sie gemeinsam. Wir besprechen einige Ihrer Ideen zum Abschluss gemeinsam!\n",
        "\n",
        "2. Können Sie Ihre Idee in einem kleinen Prototypen umsetzen? Verwenden Sie den Code dieses Notebooks, d.h.:\n",
        "  - Speichern Sie ein Dokument/ mehrere Dokumente in Ihren Drive\n",
        "  - Laden Sie die Daten mit einem geeigneten LangChain Document Loader\n",
        "  - Splitten Sie die Daten\n",
        "  - Speichern Sie die Daten in einem Vector Store\n",
        "  - Erstellen Sie einen Retriever zum Laden der relevanten Informationen aus dem Vector Store\n",
        "  - Beantworten Sie bespielhaft Fragen\n",
        "  \n",
        "  Wenn Sie keinen geeigneten Anwendungsfall oder Dokumente finden, stellen Sie sich folgendes vor: Sie sind ein Versicherungsunternehmen und haben mit Kundinnen verschiedene Kfz-Versicherungen abgeschlossen. Die Versicherungen unterscheiden sich regelmäßig im Versicherungsumfang. Für jeden Kunden liegen Ihnen die Versicherungsunterlagen vor. Sie wollen einen ChatBot entwickeln, an den sich Kundinnen wenden können. Zum Beispiel soll es möglich sein, dass Kundinnnen einen Versicherungsfall melden und der ChatBot Ihnen basierend auf den individuellen Versicherungsdokumenten relevante Informationen bereitstellt.\n",
        "\n",
        "  Sie finden unter `Business_Intelligence/Daten/RAG/Kfz_Versicherung.pdf` eine Vertragsunterlage. Stellen Sie sich vor, eine Kundin hat einen Steinschlag an der Frontscheibe Ihres Kfz und möchte wissen, ob es sich um einen Voll- oder Teilkaskoschaden handelt. Bitte nutzen Sie die OpenAI-API in Verbindung mit RAG, damit die Kundin eine angemessene Antwort erhält."
      ],
      "metadata": {
        "id": "lshTfeP6mR7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hinweise Dokumente im Google-Drive\n",
        "# 1. Speichern Sie die relevanten Dokumente in Ihren Google-Drive (WICHTIG: Merken Sie sich den Speicherort)\n",
        "# 2. Verbinden Sie dem Drive mit Ihrem Notebook\n",
        "# Google-Drive einbinden\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 3. Definieren Sie den Pfad zum Speicherort\n",
        "path = \"/content/drive/MyDrive/{SPEICHERORT DER DOKUMENTE}\"\n",
        "# 4. Laden Sie die Dokumenten"
      ],
      "metadata": {
        "id": "JBl7JQy6mSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2axX0oYmwmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai-api-zmtW_ZPs-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}