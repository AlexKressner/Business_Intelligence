{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKressner/Business_Intelligence/blob/main/openai_api_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJW63IPK7iCp"
      },
      "outputs": [],
      "source": [
        "# installation\n",
        "! pip install -q --upgrade cohere typing-extensions==4.5.0 openai tiktoken python-dotenv beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l36BZjQv7geT"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import math\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw9CZ2Bn8H8Q"
      },
      "source": [
        "# OpenAI API Keys\n",
        "Damit Sie die OpenAI API nutzen können, müssen Sie einen API Key erstellen. Mit diesem authentifizieren Sie sich, wenn Sie Anfragen an den Service stellen. Ihren **geheimen** API Key können Sie (u.a.) auf zwei Arten speichern:\n",
        "\n",
        "1. Erstellen Sie ein sogenanntes `.env`-file und speichern Sie Ihren API Key darin (`OPENAI_API_KEY={API Key}`). Aus diesem wird der Key als Umgebungsvariable mittels `dotenv` geladen. Das `.env`-file wird gelöscht, sobald die Laufzeitumgebung in Colab von Ihnen geöscht wird.\n",
        "\n",
        "2. Fügen Sie Ihren API Key direkt in eine Notebookzelle ein. ACHTUNG: Das ist definitiv kein Best-Practice. Würden Sie Ihre Colab-Datei auf github veröffentlichen, würde der API Key vermutlich recht schnell geklaut werden! Da Sie Ihre Notebook aber nirgends veröffentlichen, ist dies auch eine Option.\n",
        "\n",
        "Einen API Key können Sie [hier](https://platform.openai.com/api-keys) erstellen."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variante 1\n",
        "secret_key = \"{API Key}\""
      ],
      "metadata": {
        "id": "ziPc_B6BTGft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQQNI3utBaTW"
      },
      "outputs": [],
      "source": [
        "# Variante 2\n",
        "# Lädt die Umgebungsvariablen aus der .env-Datei\n",
        "load_dotenv()\n",
        "\n",
        "# Zugriff auf die Umgebungsvariablen\n",
        "secret_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fT5xP0X7geV"
      },
      "source": [
        "# Einstieg\n",
        "Eine ausführliche Dokumentation zur OpenAI Entwickler-Plattform finden Sie unter diesem [Link](https://platform.openai.com/docs/overview). Die genaue Spezifikation der API, die wir verwenden werden, können Sie unter folgendem [Link](https://platform.openai.com/docs/api-reference) nachschlagen.\n",
        "\n",
        "Um Anfragen über die OpenAI API zu senden, müssen Sie zunächst ein Client-Objekt erstellen (`client = OpenAI()`). Den API Key übergeben wir als Argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Ht2EfJ7geX"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=secret_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_2J2_nh7geX"
      },
      "source": [
        "Bei Verwendung der Chat-API unterscheidet man zwischen drei Rollen: **System**, **Assistent** und **Nutzer**.\n",
        "\n",
        "Beim **Assistenten** handelt es sich um das eigentliche Chat-Modell wie z.B. `gpt-3.5` oder `gpt-4`. Dieses tritt im Rahmen eines Chats (Konversation) in Interaktion mit dem **Nutzer**. Der Nutzer sendet einen Prompt an den Assistenten und dieser antwortet darauf. Das grundlegende Verhalten des Assistenten wird über die Beschreibung des **System**s gestaltet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26UjssQm7geY"
      },
      "outputs": [],
      "source": [
        "default_system_message = \"Du bist ein freundlicher und hilfsbereiter Assistent, der Kunden bei Fragen hilft\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm4Tt9Sr7geY"
      },
      "outputs": [],
      "source": [
        "# Hilfsfunktion zur Interaktion mit der Chat-API\n",
        "def get_completion(prompt, system_message=default_system_message, model=\"gpt-3.5-turbo\"):\n",
        "    system_message = [{\"role\": \"system\", \"content\": system_message}] # Wie soll sich das System grundlegend verhalten\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}] # Prompt des Nutzers\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=system_message+ messages,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xqxjp297gey"
      },
      "outputs": [],
      "source": [
        "prompt = \"Ich habe ein Problem mit meinem Smartphone und benötige Hilfe!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEKJ6X6N7gez"
      },
      "outputs": [],
      "source": [
        "get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJrk6DfW7gez"
      },
      "outputs": [],
      "source": [
        "system_message = \"Du bist ein eher mürrischer Assistent. Du sprichst möglich wenig Wörter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vDbmO-Z7ge0"
      },
      "outputs": [],
      "source": [
        "get_completion(prompt, system_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7ceEcF7ge0"
      },
      "source": [
        "# Der Chat-Endpunkt im Detail\n",
        "Der Chat-Endpunkt ist detailliert unter dem folgendem [Link](https://platform.openai.com/docs/api-reference/chat?lang=python) dokumentiert. Wir betrachten zunächst das \"chat completion object\", d.h. die Antwort des Chat-Endpunkts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJasK5yq7ge0"
      },
      "outputs": [],
      "source": [
        "prompt = \"Hallo, ich bin Alex\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmXVDGCX7ge1"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    )\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvL1Pesq7ge1"
      },
      "outputs": [],
      "source": [
        "# Attribute des ChatCompletion Objekts\n",
        "response.choices[0].message.content, response.usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y64B4jdZ7ge1"
      },
      "source": [
        "# Tokens\n",
        "\"Token\" im Zusammenhang mit Large Language Models (LLMs) wie GPT-3 oder GPT-4 beziehen sich auf die grundlegenden Einheiten der Datenverarbeitung, die das Modell verwendet, um Text zu interpretieren und zu generieren. Ein Token kann ein Wort, ein Teil eines Wortes oder sogar ein einzelnes Zeichen sein. Ein Text wird in die definierten Tokens zerlegt. Anschließend wir jeder Token in einen numerischen Wert umgewandelt. Diese Werte sind Indizes, die den entsprechenden Token im Vokabular des Modells repräsentieren.\n",
        "\n",
        "Wir verwenden hier den Byte Pair Encoding (BPE) Tokenizer [tiktoken](https://github.com/openai/tiktoken), der speziell für die Nutzung mit OpenAI Modellen entwickelt wurde. Ein BPE Tokenizer teilt Wörter in häufig vorkommende Buchstabenpaare oder Gruppen auf. Es ist besonders nützlich, um mit einem begrenzten Vokabular eine Vielzahl von Wörtern und Wortformen abzudecken.\n",
        "\n",
        "Ein ausführliches Beispiel zur Verwendung von tiktoken finden Sie [hier](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhyhEftE7ge1"
      },
      "outputs": [],
      "source": [
        "prompt = \"Hallo, meine Name ist Alexander Kressner\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUO9Q-2s7ge2"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hTrKQBY7ge2"
      },
      "outputs": [],
      "source": [
        "tokens = encoding.encode(prompt)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hy20SQ97ge2"
      },
      "outputs": [],
      "source": [
        "encoding.decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqxYNEbG7ge3"
      },
      "outputs": [],
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aBrwigU7ge3"
      },
      "outputs": [],
      "source": [
        "name = \"Alexander\"\n",
        "prompt = f\"Bitte buchstabiere meinen Namen '{name}' rückwärts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXojm_He7ge3"
      },
      "outputs": [],
      "source": [
        "encoding.encode(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5jdhsdE7ge3"
      },
      "outputs": [],
      "source": [
        "get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcvm-DK-7ge3"
      },
      "outputs": [],
      "source": [
        "name = \"A-l-e-x-a-n-d-e-r\"\n",
        "prompt = f\"Bitte buchstabiere meinen Namen '{name}' rückwärts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGrnk8-i7ge4"
      },
      "outputs": [],
      "source": [
        "tokens = encoding.encode(name)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk9TBGQf7ge4"
      },
      "outputs": [],
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSYkjRNi7ge4"
      },
      "outputs": [],
      "source": [
        "get_completion(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MepSZVenAAh"
      },
      "source": [
        "**FRAGE**\n",
        "Wie viele Token hat der Satz `\"Ich studiere an der Dualen Hochschule Baden-Württemberg!\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gWkTiIvnOpA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXBq3p27ge4"
      },
      "source": [
        "## Argumente für den Chat-Endpunkt\n",
        "Wenn Sie Anfragen an den [Chat-Endpunkt](https://platform.openai.com/docs/api-reference/chat) stellen, können Sie verschiedene Argumente übergeben. Die wichtigsten wollen wir einmal näher betrachten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwhQY1RP7ge4"
      },
      "outputs": [],
      "source": [
        "prompt = \"Hallo, meine Name ist Alex\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT0izKSm7ge4"
      },
      "outputs": [],
      "source": [
        "default_keyword_args = {\n",
        "    \"model\":\"gpt-3.5-turbo\",\n",
        "    \"messages\":[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOx-U26z7ge5"
      },
      "source": [
        "### `logprobs`\n",
        "Wenn diese Option aktiviert ist (also \"true\"), dann gibt das System die logarithmierten Wahrscheinlichkeiten jedes einzelnen Ausgabetokens, die im Inhalt der Nachricht enthalten sind, zurück."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARNtLQwV7ge5"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"logprobs\":True}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSQxplNI7ge5"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD3csGf77ge5"
      },
      "outputs": [],
      "source": [
        "response.choices[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_hBJQIk7ge5"
      },
      "outputs": [],
      "source": [
        "math.exp(response.choices[0].logprobs.content[0].logprob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxQyQOca7ge5"
      },
      "outputs": [],
      "source": [
        "{Token.token:math.exp(Token.logprob) for Token in response.choices[0].logprobs.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPOejwqB7ge5"
      },
      "source": [
        "### `top_logprobs`\n",
        "Beschreibt einen Parameter, der einen ganzzahligen Wert zwischen 0 und 5 annimmt. Dieser Wert spezifiziert die Anzahl der wahrscheinlichsten Tokens, die an jeder Token-Position zurückgegeben werden sollen, wobei jedem Token eine zugehörige logarithmierte Wahrscheinlichkeit beigefügt ist. Der Parameter `logprobs` muss auf `true` gesetzt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdDJsM817ge5"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"logprobs\":True, \"top_logprobs\":3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tYb4kpj7ge5"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txaOATsC7ge6"
      },
      "outputs": [],
      "source": [
        "response.choices[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9VTqrFn7ge6"
      },
      "outputs": [],
      "source": [
        "toplogs = {TopLog.token:math.exp(TopLog.logprob) for TopLog in response.choices[0].logprobs.content[0].top_logprobs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru_Y8QLf7ge6"
      },
      "outputs": [],
      "source": [
        "toplogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYMCT-dV7ge6"
      },
      "source": [
        "### `logit_bias`\n",
        "Ändere die Wahrscheinlichkeit des Erscheinens bestimmter Tokens in der Vervollständigung. Die Bias-Werte reichen von -100 bis 100. Negative/Positive Werte verringern/erhöhen die Wahrscheinlichkeit der Auswahl. Werte von -100 oder 100 sollten zu einem Verbot des relevanten Tokens führen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldR6CCV47ge6"
      },
      "outputs": [],
      "source": [
        "token = encoding.encode(\"Hallo\")\n",
        "token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYmYPuIb7ge6"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"logit_bias\":{token[0]:-100}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXRaQQai7ge7"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsD5rTeInZay"
      },
      "source": [
        "**FRAGE**\n",
        "\n",
        "1) Wie können Sie weitere Begrüßungswörter verhindern?\n",
        "\n",
        "2) Wie stellen Sie sicher, dass ein Wort in der Antwort des Assistenten erscheint?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b3cgkBFn44m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWsUjGIn7ge7"
      },
      "source": [
        "### `max_tokens`\n",
        "Die maximale Anzahl an Tokens, die in der Chat-Vervollständigung generiert werden können. Die Gesamtlänge der Eingabetokens und der generierten Tokens wird durch die Kontextlänge des Modells begrenzt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKIs8gRW7ge7"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "      Anzahl input token gesamt = {response.usage.prompt_tokens} \\n\n",
        "      Anzahl output token gesamt = {response.usage.completion_tokens} \\n\n",
        "      Anzahl token gesamt = {response.usage.total_tokens}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faSmwk8G7ge7"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"max_tokens\":10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poObOR6v7ge7"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbZVy6OG7ge7"
      },
      "source": [
        "### `response_format`\n",
        "Spezifiziert das Ausgabeformat des Modells (JSON oder Text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe7oAOMb7ge7"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EDIMNer7ge7"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\n",
        "    \"response_format\":{\"type\": \"json_object\"},\n",
        "    \"model\":\"gpt-3.5-turbo-1106\",\n",
        "    \"messages\":[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and answer in json format.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_Yi81lc7ge7"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwT74c6O7ge7"
      },
      "outputs": [],
      "source": [
        "response_dict = json.loads(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMsTFnwq7ge8"
      },
      "outputs": [],
      "source": [
        "response_dict[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ6uXhbP7ge8"
      },
      "source": [
        "### `temperatur`\n",
        "Der Parameter kann Werte zwischen 0 und 2 annehmen. Höhere Werte wie 0,8 machen die Ausgabe zufälliger, während niedrigere Werte wie 0,2 sie fokussierter und deterministischer machen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of-UyXgA7ge8"
      },
      "outputs": [],
      "source": [
        "prompt = \"Bitte schreib mir ein Gedicht, dass Studierende motiviert der Vorlesung zur künstlichen Intelligenz aufmerksam zu folgen und aktiv teilzunehmen! Bitte schreibe maximal 30 Wörter!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_uUBVZR7ge8"
      },
      "outputs": [],
      "source": [
        "default_keyword_args = {\n",
        "    \"model\":\"gpt-3.5-turbo\",\n",
        "    \"messages\":[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SACbBMhx7ge8"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"temperature\":0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCLS0eJb7ge8"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvInXzjG7ge8"
      },
      "outputs": [],
      "source": [
        "keyword_args = default_keyword_args | {\"temperature\":1.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26g2kgQg7ge8"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(**keyword_args)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPOzzEVl7ge8"
      },
      "source": [
        "# Beispiel: Web-Scapping + Chat API\n",
        "Wir wollen in nachfolgenden Beispiel Daten von einer Website laden (scrappen) und mittels Chat-API analysieren. Wir nutzen dafür die Bibliothek `requests` und `BeautifulSoup`. Das Python-Paket `requests` ist eine benutzerfreundliche Bibliothek, die das Senden von HTTP-Anfragen ermöglicht und eine einfache Verwendung für den Zugriff auf Web-Ressourcen bietet. `BeautifulSoup` ist eine Python-Bibliothek, die zur Analyse und Extraktion von Daten aus HTML- und XML-Dateien dient und dabei eine einfache Schnittstelle für das Parsen und Navigieren im Dokumentenbaum bietet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsagXGPr7ge9"
      },
      "outputs": [],
      "source": [
        "def get_website_text(url: str):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Überprüfen, ob die Anfrage erfolgreich war\n",
        "    if response.status_code == 200:\n",
        "        # Parsen des HTML-Inhalts der Seite\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extrahieren sämtlichen Textes der Webseite\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    else:\n",
        "        # Zurückgeben einer Fehlermeldung mit dem HTTP-Statuscode\n",
        "        return f\"Fehler beim Abrufen der Webseite: HTTP-Statuscode {response.status_code}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKhj6l2c7ge9"
      },
      "outputs": [],
      "source": [
        "text = get_website_text(\"https://www.lappcareer.com/stellenangebote.html?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha-flqMK7ge9"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMm0YprH7ge9"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, system_message=default_system_message, model=\"gpt-3.5-turbo\"):\n",
        "    system_message = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=system_message+ messages,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdhMV9dQ7ge9"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Du erhälst nachfolgend einen Text, der die ausgeschriebenen Stellen eines Unternehmens auf seiner Website wiedergibt!\n",
        "Gibt es Stellenausschreibungen für ein Duales Studium? Falls ja, bitte nenne die Ausschreibungen!\n",
        "Der relevante Text folgt nach dem Doppelpunkt: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfmWL_-j7ge9"
      },
      "outputs": [],
      "source": [
        "print(get_completion(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frage**\n",
        "Wie gut funktioniert die Suche nach anderen Stellenausschreibungen, wenn Sie ein anderen \"Such-Prompt\" verwenden?"
      ],
      "metadata": {
        "id": "M8AecrBN7tKa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSQiRvuE7slx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSbvoyZwlYpB"
      },
      "source": [
        "# Aufgabe: Witzemaschine\n",
        "Sie alle kennen schlechten Wortwitze wie zum Beispiel: `Was ist grün und klopft an die Tür?` - Antwort: `Ein Klopfsalat!`. Versuchen Sie mit Hilfe von Few-Shot-Prompting eine Witze-Maschine zu programmieren, die auf eine Frage eines Nutzers mit einem Wortwitz antwortet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npaAaWp1DMzm"
      },
      "outputs": [],
      "source": [
        "default_system_message = \"Du bist ein KI-System, das Wortwitze erstellt. Du antwortest dabei bitte in einer konsistenten Art und Weise.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Frage =[\n",
        "    \"<Frage>: ...?\",\n",
        "    \"<Frage>: ...?\",\n",
        "]"
      ],
      "metadata": {
        "id": "4qnCccLq13Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Antwort =[\n",
        "    \"<Antwort>: ...!\",\n",
        "    \"<Antwort>: ...!\",\n",
        "]"
      ],
      "metadata": {
        "id": "xEGEOlwq2c-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "for example in range(len(Frage)):\n",
        "  examples += [{\"role\": \"user\", \"content\": Frage[example]}] + [{\"role\": \"assistant\", \"content\": Antwort[example]}]\n"
      ],
      "metadata": {
        "id": "TUIbVk-y2vpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<Frage>: ...?\""
      ],
      "metadata": {
        "id": "EKkDNbuO3P3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbSaGdCJrgAv"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, system_message=default_system_message, model=\"gpt-3.5-turbo\"):\n",
        "    system_message = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    user_message = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=system_message + examples + user_message,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "id": "E4ESEgmt3NWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beispiel: Bewerber-Maschine"
      ],
      "metadata": {
        "id": "nTUehAEb4YOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXjzS9oGD_Uj"
      },
      "outputs": [],
      "source": [
        "def get_website_text(url: str):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Überprüfen, ob die Anfrage erfolgreich war\n",
        "    if response.status_code == 200:\n",
        "        # Parsen des HTML-Inhalts der Seite\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extrahieren sämtlichen Textes der Webseite\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    else:\n",
        "        # Zurückgeben einer Fehlermeldung mit dem HTTP-Statuscode\n",
        "        return f\"Fehler beim Abrufen der Webseite: HTTP-Statuscode {response.status_code}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "242EL25pD_Uj"
      },
      "outputs": [],
      "source": [
        "def get_website_urls(url: str):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Überprüfen, ob die Anfrage erfolgreich war\n",
        "    if response.status_code == 200:\n",
        "        # Parsen des HTML-Inhalts der Seite\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extrahieren sämtlichen Textes der Webseite\n",
        "        hyperlinks = soup.find_all('a')\n",
        "        return [link.get('href') for link in hyperlinks if link.get('href')]\n",
        "    else:\n",
        "        # Zurückgeben einer Fehlermeldung mit dem HTTP-Statuscode\n",
        "        return f\"Fehler beim Abrufen der Webseite: HTTP-Statuscode {response.status_code}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rshV80kdD_Uk"
      },
      "outputs": [],
      "source": [
        "def get_link_by_substring(strings: [str], substrings: [str]):\n",
        "    # Konvertierung der Substrings in Kleinbuchstaben für case-insensitive Suche\n",
        "    substrings_lower = [substring.lower() for substring in substrings]\n",
        "\n",
        "    # Filtern der Liste durch Überprüfung, ob irgendein Substring in jedem Element enthalten ist\n",
        "    filtered_list = [s for s in strings if any(substring_lower in s.lower() for substring_lower in substrings_lower)]\n",
        "\n",
        "    return filtered_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1fhsjs9D_Uk"
      },
      "outputs": [],
      "source": [
        "# Hilfsfunktion zur Interaktion mit der Chat-API\n",
        "def get_completion(messages: [str], model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg3pTxJ-D_Ul"
      },
      "outputs": [],
      "source": [
        "applicant_profile = \"\"\"\n",
        "Name des Bewerbers ist Max Mustermann; abgeschlossenes Abitur mit Note 1,7; Leistungskurse Mathematik und Physik;\n",
        "Interesse an digitalen Geschäftsmodellen im Handel; Erstes Praktikum in einem Startup, das über einen online-shop schmuck verkauft;\n",
        "Programmiererfahrung in Python, insbesondere Webprogrammierung; motiviert und zuverlässig\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jOjJ_JnD_Ul"
      },
      "outputs": [],
      "source": [
        "system_message = f\"\"\"\n",
        "Du bist ein Experte bei der Erstellung professioneller Anschreiben für Bewerbungen! Du erstellst ein Anschreiben für das in einfachen Anführungszeichen beschriebene\n",
        "Bewerberprofil.\n",
        "\n",
        "'{applicant_profile}'\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSsb5F_hD_Ul"
      },
      "outputs": [],
      "source": [
        "def add_job_description_to_prompt(job_description: str):\n",
        "    prompt = f\"\"\"\n",
        "    Du erhälst in einfachen Anführungszeichen einen Ausschreibungstext für ein duales Studium. Bitte verwende das Bewerberprofil, um ein Anschreiben für eine Bewerbung\n",
        "    auf die Stellenausschreibung zu erstellen. Das Anschreiben soll professionell geschrieben sein. Das Anschreiben soll spezifisch auf die Anforderungen in der\n",
        "    Ausschreibung eingehen und zeigen, dass der Bewerber diese bestmöglich erfüllt. Die Länge des Anschreibens soll eine DIN-A4 Seite sein.\n",
        "\n",
        "    '{job_description}'\n",
        "    \"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZV6qVTQD_Ul"
      },
      "outputs": [],
      "source": [
        "links = get_website_urls(\"https://www.lappcareer.com/stellenangebote.html?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMzbyuIkD_Um"
      },
      "outputs": [],
      "source": [
        "relevant_links = get_link_by_substring(links,[\"duales studium\",\"duales-studium\", \"dual\", \"dhbw\"])\n",
        "print(f\"{len(relevant_links)} passende Stellenausschreibungen gefunden!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sOHw74QD_Um"
      },
      "outputs": [],
      "source": [
        "job_descriptions = []\n",
        "for link in relevant_links:\n",
        "    job_descriptions.append(get_website_text(link))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9a-4TnkD_Um"
      },
      "outputs": [],
      "source": [
        "cover_letter = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU00Odt0D_Un"
      },
      "outputs": [],
      "source": [
        "for job in job_descriptions:\n",
        "    messages =  [\n",
        "    {'role':'system',\n",
        "    'content': system_message},\n",
        "    {'role':'user',\n",
        "    'content': add_job_description_to_prompt(job)},\n",
        "    ]\n",
        "    cover_letter.append(get_completion(messages))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cover_letter[0])"
      ],
      "metadata": {
        "id": "1tY4v9t_Fbq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jiz8cifm7R_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "openai-api-zmtW_ZPs-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}